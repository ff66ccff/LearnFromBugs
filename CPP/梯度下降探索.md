# 梯度下降探索
初始条件是5个加了随机噪声的Y=5X+2的点，学习率0.01，迭代次数1000，代码为
```
#include <iostream>
#include <vector>
#include <version>
using namespace std;


int main() {
    const vector<double> DataX = {1.0, 2.0, 3.0, 4.0, 5.0};
    const vector<double> DataY = {6.9, 12.2, 17.1, 21.8, 27.3}; // 随机噪声后的Y=5X+2
    double Loss = 0; // 用于计算预测值减真实值的平方和
    double k = 0;
    double b = 0; // 对于新直线Y=kX+b的初始参数
    const int iterations = 1000; // 迭代次数
    double X;
    double Y_predict;
    double Y_true; // 声明变量
    cout << "初始参数为:k=" << k << "  b=" << b << endl;
    // 循环迭代拟合直线
    for (int i = 0; i < iterations; i++) {
        double learning_rate = 0.01; // 学习率
        double delta; // 后续计算的中间值
        double k_sum = 0;
        double b_sum = 0;
        for (int j = 0; j < DataX.size(); j++) {
            double X = DataX[j]; // 取出X的值，这是客观值所以不需要预测
            double Y_predict = k * DataX[j] + b; // 计算预测值
            double Y_true = DataY[j]; // 拿出真实值
            double delta = Y_predict - Y_true;
            Loss += delta * delta; // 计算预测值减真实值的平方和
            k_sum += delta * X;
            b_sum += delta;
        }
        double k_avg = k_sum / DataX.size();
        double b_avg = b_sum / DataX.size();
        k = k - 2 * learning_rate * k_avg; // 迭代后的K
        b = b - 2 * learning_rate * b_avg; // 迭代后的b
        cout << "第" << i + 1 << "次迭代，本次Loss:" << Loss << "  本次k=" << k << "  b=" << b << endl;
        Loss = 0; // 清除Loss值，为下一次迭代作准备
    }
}

```
此时运行的最终结果为
> 第1000次迭代，本次Loss:0.156268  本次k=5.04473  b=1.92291

在此基础上，把学习率调小为0.001（原来是0.01），迭代1000次后发现Loss变大了，也就是更不精确。
> 第1000次迭代，本次Loss:0.27421  本次k=5.13979  b=1.57972
进一步调整，学习率0.001，迭代次数10000，比之前更精确，但是仍然不如初始值。
> 第10000次迭代，本次Loss:0.156269  本次k=5.04476  b=1.92282
那要是在学习率0.01，迭代次数10000的情况下呢？更精确了，但是在运行结果中注意到，经过某个迭代轮次之后，迭代中的k和b是没有变化的，所以盲目增加迭代次数不一定能提升精确度。
> 第10000次迭代，本次Loss:0.156  本次k=5.04  b=1.94
现在我们回到最初的情况，学习率0.01，迭代次数1000，我们把初始参数改为k=2,b=4。此时虽然仍然导致了更不精确，但是对最终结果的影响是有限的。
> 第1000次迭代，本次Loss:0.163613  本次k=5.01475  b=2.03115
现在我们回到最初的情况，学习率0.01，迭代次数1000，把学习率改成每10个迭代就减少为原来的1/2,反映在代码上，就是
```
        if ((i+1)%10==0) {
            learning_rate=learning_rate/2;
        }
```
现在再跑一遍，和初始值一样？
> 第1000次迭代，本次Loss:0.156268  本次k=5.04473  b=1.92291
现在更改Loss函数的策略，改为L1形式
```
#include <iostream>
#include <vector>
#include <version>
using namespace std;


int main() {
    const vector<double> DataX = {1.0, 2.0, 3.0, 4.0, 5.0};
    const vector<double> DataY = {6.9, 12.2, 17.1, 21.8, 27.3}; // 随机噪声后的Y=5X+2
    double Loss = 0; // 用于计算预测值减真实值的平方和
    double k = 0;
    double b = 0; // 对于新直线Y=kX+b的初始参数
    const int iterations = 1000; // 迭代次数
    double X;
    double Y_predict;
    double Y_true; // 声明变量
    cout << "初始参数为:k=" << k << "  b=" << b << endl;
    // 循环迭代拟合直线
    for (int i = 0; i < iterations; i++) {
        double learning_rate = 0.1; // 学习率
        double delta; // 后续计算的中间值
        double k_sum = 0;
        double b_sum = 0;
        for (int j = 0; j < DataX.size(); j++) {
            double X = DataX[j]; // 取出X的值，这是客观值所以不需要预测
            double Y_predict = k * DataX[j] + b; // 计算预测值
            double Y_true = DataY[j]; // 拿出真实值
            double delta = Y_predict - Y_true;
            Loss += abs(delta) ; // 计算预测值减真实值的平方和
            double sign=0;
            if (delta>1e-6) {
                sign = 1;
            }else if (delta<-1e-6) {
                sign = -1;
            }
            k_sum += sign * X;
            b_sum += sign;
        }
        double k_avg = k_sum / DataX.size();
        double b_avg = b_sum / DataX.size();
        k = k - learning_rate * k_avg; // 迭代后的K
        b = b - learning_rate * b_avg; // 迭代后的b
        cout << "第" << i + 1 << "次迭代，本次Loss:" << Loss << "  本次k=" << k << "  b=" << b << endl;
        Loss = 0; // 清除Loss值，为下一次迭代作准备
    }
}

```
注意到此时学习率应该稍微加大一点，不然结果很鬼畜。运行后发现迭代二三十次就出现了结果循环的现象。
> 第1000次迭代，本次Loss:2.9  本次k=5.24  b=1.76
# 综上
**学习率 (Learning Rate) 是最重要的超参数：**
太小 (0.001)：收敛速度极慢。在相同的迭代次数下，模型远未达到最优解，导致结果更差。
合适 (0.01)：能在合理的迭代次数内高效地找到一个非常好的解。
**迭代不是越多越好，关键在于“收敛”：**
当模型的参数（k 和 b）和损失（Loss）在多次迭代后不再有明显变化时，我们就说模型已经收敛了。
一旦收敛，继续增加迭代次数是无效的，只会浪费计算资源，并不能提升模型精度。
**损失函数的选择从根本上改变了优化过程：**
L2 损失 (平方误差)：它的梯度与误差大小成正比。当接近最优解时，误差变小，梯度也随之变小，这让模型可以“平滑减速”，稳定地收敛到最小值。
L1 损失 (绝对值误差)：它的梯度大小是恒定的（只要误差不为零）。这就像一个没有刹车的车，在接近最优解时也不会自动减速，因此很容易在最小值附近来回“振荡”，难以精确停下。这也解释了为什么 L1 的结果会出现循环现象。